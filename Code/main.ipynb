{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BLS CPI Data Scraper**\n",
    "\n",
    "Description: This will pull Urban Consumer CPI Data from BLS using its json based API.\n",
    "Below are some references:\n",
    "\n",
    "* [FUll ID CREATION](https://www.bls.gov/help/hlpforma.htm#id=CU)\n",
    "* [AREA CODES](https://download.bls.gov/pub/time.series/cu/cu.area)\n",
    "* [ITEM CODES](https://download.bls.gov/pub/time.series/cu/cu.item)\n",
    "* [MANUAL DOWNLOAD](https://www.bls.gov/cpi/data.htm)\n",
    "* [API INFORMATION](https://www.bls.gov/developers/api_faqs.htm#register1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup\n",
    "\n",
    "1. Packages\n",
    "2. Directories\n",
    "3. Helper Functions\n",
    "4. Setup Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import logging.handlers\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2 Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: All directories the program used should be included as a global variable here\n",
    "MAIN_DIR =  \"D:\\\\Code\\\\PYTHON\\\\BLS_SCRAPER\\\\\"\n",
    "DATA_DIR = MAIN_DIR + f\"Data\"\n",
    "# NOTE: Automatic Log Folder directory creation based on date.\n",
    "# NOTE: The file iteself is created based on the time. \n",
    "LOG_DIR = MAIN_DIR + f\"Log\\\\{datetime.now().strftime('%Y%m%d')}\\\\\" \n",
    "LOG_FILE = LOG_DIR + f\"Log_{datetime.now().strftime('%H%M%S')}.log\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_setup(dir_list):\n",
    "    '''\n",
    "    DESCRIPTION > If the directory does not exist it will create it\n",
    "    '''\n",
    "    for directory in dir_list:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "def logging_setup():\n",
    "    '''\n",
    "    DESCRIPTION -> Setups the logging file for code\n",
    "    '''\n",
    "    try:\n",
    "      handler = logging.handlers.WatchedFileHandler(os.environ.get(\"LOGFILE\", LOG_FILE))\n",
    "      formatter = logging.Formatter(fmt=\"%(asctime)s %(levelname)-8s %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "      handler.setFormatter(formatter)\n",
    "      logging.getLogger().handlers.clear()\n",
    "      root = logging.getLogger()\n",
    "      root.setLevel(os.environ.get(\"LOGLEVEL\", \"INFO\"))\n",
    "      root.addHandler(handler)\n",
    "      logging.propogate = False\n",
    "      logging.info(\"Log File was created successfully.\")\n",
    "    except Exception as e:\n",
    "        exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: All steps regrading setup should be completed here\n",
    "DIR_LIST = [MAIN_DIR, LOG_DIR, DATA_DIR]\n",
    "directory_setup(DIR_LIST)\n",
    "logging_setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: BLS Scraping Class\n",
    "\n",
    "Description: This is the main method used to scrape the information off of BLS.\n",
    "\n",
    "Contains the methods\n",
    "* get_data\n",
    "* process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------DESCRIPTION--------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Passes the BLS json request and gets the data. \n",
    "Afterwards it processes the data and enriches the data with some additional information about area and item names.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "-----------------------------------------------PARAMETERS---------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "api_key -> API_KEY for BLS data queries\n",
    "out_file -> Location for Data to be outputted\n",
    "series_id -> All the series of CPI data that you want\n",
    "start_year -> Query start range\n",
    "end_year -> Query end range\n",
    "area_df -> Dataframe containing information on metro area codes and names\n",
    "item_df -> Dataframe containing information on item codes and names\n",
    "\n",
    "'''\n",
    "class bls_data_scraper:\n",
    "\n",
    "    def __init__(self, api_key, out_file, series_id, start_year, end_year, area_df, item_df):\n",
    "        headers = {\"Content-type\": \"application/json\"}\n",
    "        parameters = json.dumps({\n",
    "                                \"seriesid\":series_id, \n",
    "                                \"startyear\":start_year, \n",
    "                                \"endyear\":end_year, \n",
    "                                \"registrationkey\":api_key\n",
    "                                })\n",
    "        self.area_df = area_df\n",
    "        self.item_df = item_df\n",
    "        \n",
    "        # Requests the data from BLS\n",
    "        json_data = self.get_data(headers, parameters)\n",
    "        # Processes the data from BLS\n",
    "        df_data = self.process_data(json_data, area_df, item_df)\n",
    "        # Converts the data to an array to write -> Need to do this so that we have a single header\n",
    "        list_df_data = df_data.values.tolist()\n",
    "\n",
    "        # WARNING: One issue if you kill the run and start again it will continue to append to the same file\n",
    "\n",
    "        # Writes the cleaned up data into the specified out_file\n",
    "        with open(out_file , \"a\") as file:\n",
    "            headers = [\"ID\",\"area_name\",\"item_name\",\"year\",\"period\",\"periodName\",\"latest\",\"value\",\"footnotes\"]\n",
    "            writer = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "            if os.stat(out_file).st_size==0:\n",
    "                writer.writerow(headers)\n",
    "            for row in list_df_data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "    def get_data(self, headers, parameters):\n",
    "        '''\n",
    "        DESCRIPTION -> Posts the url and we get the data back in a json format\n",
    "\n",
    "        PARAM 1 -> headers -> self.header a BLS API requirement\n",
    "        PARAM 2 -> parameters -> The data specification that you plan on querying\n",
    "        '''\n",
    "        post = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=parameters, headers=headers)\n",
    "        json_data = json.loads(post.text)\n",
    "        return json_data\n",
    "    \n",
    "\n",
    "    def process_data(self, json_data, area_df, item_df):\n",
    "        '''\n",
    "        DESCRIPTION -> Cleans and enriches the JSON data that we just processed\n",
    "\n",
    "        PARAM 1 -> json_data -> The raw JSON data we pulled from BLS\n",
    "        PARAM 2 -> area_df -> The area code and name information\n",
    "        PARAM 3 -> item_df -> The item code and name information\n",
    "        '''\n",
    "\n",
    "        # NOTE: A lot of the data is stored inside multi-layed dictionaries/lists\n",
    "        # All the information is stored under a three layer depth\n",
    "        df = pd.json_normalize(json_data, record_path=[\"Results\", \"series\", \"data\"], meta=[[\"Results\", \"series\", \"seriesID\"]])\n",
    "        df.rename(columns = {\"Results.series.seriesID\":\"ID\"}, inplace = True)\n",
    "\n",
    "        # Parsing out the area_code and item_code from the entirety of the ID that we generated\n",
    "        df[\"area_code\"] = df[\"ID\"].apply(lambda x: x[4:8])\n",
    "        df[\"item_code\"] = df[\"ID\"].apply(lambda x: x[8:])\n",
    "\n",
    "        # Enriching the data here\n",
    "        df = pd.merge(df, area_df, how=\"left\", on=\"area_code\")\n",
    "        df = pd.merge(df, item_df, how=\"left\", on=\"item_code\")\n",
    "        df.drop(columns=[\"area_code\", \"item_code\"], inplace=True)\n",
    "\n",
    "        # rearrange column ordering\n",
    "        name_list = df.columns.tolist()\n",
    "        name_list = name_list[-3:-2] + name_list[-2:-1] + name_list[-1:] + name_list[:-3]\n",
    "        df = df[name_list]\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: BLS Area and Item Information Scraper\n",
    "\n",
    "1. bls_code_scraper function \n",
    "2. Get and Filter the Information\n",
    "3. Generate List of BLS Codes from Filtered Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.1 bls_code_scraper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bls_code_scraper(url, file_dir):\n",
    "    '''\n",
    "    DESCRIPTION -> Scrapes textual information off of BLS links\n",
    "\n",
    "    PARAM 1 -> file_dir -> Where you want to write the raw_information\n",
    "    '''\n",
    "\n",
    "    response = requests.get(url)\n",
    "    content = response.content.decode(\"utf-8\")\n",
    "    area_content = csv.reader(content.splitlines(), delimiter=\"\\t\")\n",
    "    area_list = list(area_content)\n",
    "\n",
    "    # n is the number of elements that we want to remove from the back of the list\n",
    "    n = 3\n",
    "    # Writing the uncleaned relavant information into file \n",
    "    with open(file_dir, \"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in area_list:\n",
    "            row = row[: len(row) - n]\n",
    "            writer.writerow(row)\n",
    "    file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2 Get and Filter the Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the area code information\n",
    "bls_code_scraper(\"https://download.bls.gov/pub/time.series/cu/cu.area\", f\"{DATA_DIR}\\\\raw_area.csv\")\n",
    "# Scraping the item code information\n",
    "bls_code_scraper(\"https://download.bls.gov/pub/time.series/cu/cu.item\", f\"{DATA_DIR}\\\\raw_item.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_df = pd.read_csv(f\"{DATA_DIR}\\\\raw_area.csv\")\n",
    "\n",
    "# Filter for S area_code, since we only want metro area codes\n",
    "area_df[\"code_check\"] = area_df[\"area_code\"].apply(lambda x: \"Pass\" if re.match(\"S[0-9]{3}\", x) else \"Fail\")\n",
    "area_df = area_df[area_df[\"code_check\"] == \"Pass\"]\n",
    "\n",
    "# Remove the area_names that have Size Class A in their name. Those are irrelavent.\n",
    "area_df[\"name_check\"] = area_df[\"area_name\"].apply(lambda x: \"Pass\" if \"Size Class A\" not in x else \"Fail\")\n",
    "area_df = area_df[area_df[\"name_check\"] == \"Pass\"]\n",
    "\n",
    "area_df.drop(columns=[\"code_check\", \"name_check\"], inplace=True)\n",
    "area_df.to_csv(f\"{DATA_DIR}\\\\clean_area.csv\", index=False)\n",
    "\n",
    "\n",
    "item_df = pd.read_csv(f\"{DATA_DIR}\\\\raw_item.csv\")\n",
    "# Only include item_codes that have a shorter length than 4 since those are the more aggregated values.\n",
    "item_df[\"code_check\"] = item_df[\"item_code\"].apply(lambda x: \"Pass\" if len(x)<4 else \"Fail\")\n",
    "item_df = item_df[item_df[\"code_check\"] == \"Pass\"]\n",
    "\n",
    "# If the item_name contains All Items we remove since those are too aggregated\n",
    "item_df[\"name_check\"] = item_df[\"item_name\"].apply(lambda x: \"Pass\" if not re.match(\"All items [a-zA-Z\\s,]*\",x) else \"Fail\")\n",
    "item_df = item_df[item_df[\"name_check\"] == \"Pass\"]\n",
    "\n",
    "item_df.drop(columns=[\"code_check\", \"name_check\"], inplace=True)\n",
    "item_df.to_csv(f\"{DATA_DIR}\\\\clean_item.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3 Generate List of BLS Codes from Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here is a sample of how the BLS creates the unique identifier. You can find more information on it here (https://www.bls.gov/help/hlpforma.htm#id=CU).\n",
    "\n",
    "\tSeries ID    CUUR0000SA0L1E\n",
    "\tPositions       Value           Field Name\n",
    "\t1-2             CU              Prefix\n",
    "\t3               U               Not Seasonal Adjustment Code\n",
    "\t4               R               Periodicity Code\n",
    "\t5-8             0000            Area Code\n",
    "\t9               S               Base Code\n",
    "\t10-16           A0L1E           Item Code\n",
    "\"\"\"\n",
    "code_list = []\n",
    "prefix = \"CU\"\n",
    "# This dataset does not have any seasonally adjusted data\n",
    "seasonality = \"U\"\n",
    "# R stands for monthly\n",
    "periodicity = \"R\"\n",
    "for area in area_df[\"area_code\"]:\n",
    "    for item in item_df[\"item_code\"]:\n",
    "        code_list.append(f\"{prefix}{seasonality}{periodicity}{str(area)}{str(item)}\")\n",
    "logging.info(f\"Total Unique Codes: {len(code_list)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Limit of 500 calls per key.\n",
    "Can make 50 calls every query.\n",
    "'''\n",
    "# Drexel API Key: \"024f5a0ca6e7494cbec2ea4088cd4a9d\"\n",
    "# GMAIL API Key: \"73df4bb81189431089fe2f247af35ce1\"\n",
    "api_key = \"73df4bb81189431089fe2f247af35ce1\"\n",
    "start_year = 2019\n",
    "end_year = 2022\n",
    "for x in range(0, len(code_list), 50):\n",
    "    code_chunk = code_list[x:x+50]\n",
    "    bls_data_scraper(api_key, f\"{DATA_DIR}\\\\bls_data.csv\", code_chunk, start_year, end_year, area_df, item_df)\n",
    "    time.sleep(2)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85fec92199cc14a2bd4b3d9b9709b8077b8d4ba9daddeb167bcfeac63be00291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
